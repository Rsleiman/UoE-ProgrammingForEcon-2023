{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "879c7fef",
   "metadata": {},
   "source": [
    "# Problem Set 5: Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8232537f",
   "metadata": {},
   "source": [
    "## Question 1: Rosenbrock Constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700e05cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import numdifftools as nd\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede99ff7",
   "metadata": {},
   "source": [
    "### 1(a) Sequential Quadratic Programming with Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa36590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Adding a, b into arguments\"\"\"\n",
    "def rosen(X, a, b):     # Objective function\n",
    "    f = []\n",
    "    for i in range(len(X)-1):   # Iterate over N-1 variables in X\n",
    "        f_i = ((a*(1-X[i])**2) + b*((X[i+1] - X[i]**2))**2)\n",
    "        f.append(f_i)\n",
    "    return sum(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872814ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "### PART A ###\n",
    "##############\n",
    "\n",
    "\"\"\"\n",
    "Throughout part A, we use the following arguments for testing purposes:\n",
    "X = [1,2,3]\n",
    "lambda = 1\n",
    "a = 1\n",
    "b = 1\n",
    "r = 1\n",
    "\"\"\"\n",
    "Y = [ 1,  2,   3,   1]\n",
    "#     ^   ^    ^    ^  \n",
    "#   X[0] X[1] X[2] lam\n",
    "a = 1\n",
    "b = 1\n",
    "r = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96035ce7",
   "metadata": {},
   "source": [
    "#### i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2145f86a",
   "metadata": {},
   "source": [
    "<h2><center>$\\mathcal{L}$ = f(x) -  $\\lambda$$\\cdot$g(x)</center></h2>\n",
    "\n",
    "<h2><center>f(x) = $\\sum_{i=1}^{n-1}{(a(1-x_i}^2) + b(x_{i+1}-x^2_i)^2)$</center></h2>\n",
    "<h2><center>g(x) = $\\sum_{i=1}^{n}{x^2_i}  - r$</center></h2>\n",
    "\n",
    "\n",
    "<h2><center>$\\mathcal{L}$ = $\\sum_{i=1}^{n-1}{(a(1-x_i}^2) + b(x_{i+1}-x^2_i)^2)$ - $\\lambda$$\\cdot$$(\\sum_{i=1}^{n}{x^2_i}  - r)$</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eac17b",
   "metadata": {},
   "source": [
    "#### ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a24268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ ii\n",
    "def constraint(X, r):\n",
    "    g = []\n",
    "    for i in range(0, len(X)):  # Iterate over N variables in X\n",
    "        g_i=(X[i])**2\n",
    "        g.append(g_i)\n",
    "    return sum(g) - r           # Minus r at the end to complete constraint expression\n",
    "\n",
    "\"\"\"To be able to use the gradient functions in later questions, we combine the x variables and lambda into one argument vector, where the last variable is lambda\"\"\"\n",
    "def lagrangian(Y, a, b, r):\n",
    "    X = Y[0:-1]\n",
    "    lam = Y[-1]\n",
    "    L = rosen(X, a, b) - lam*(constraint(X, r))\n",
    "    return L \n",
    "\n",
    "# EXAMPLE:\n",
    "lagrangian(Y,a,b,r)     # ANSWER: -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6052f",
   "metadata": {},
   "source": [
    "#### iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50490abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.,   8.,  -8., -13.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ iii\n",
    "# From numdifftools, we can use the Hessian function\n",
    "def gradient(Y,a,b,r):\n",
    "    return nd.core.Jacobian(lagrangian)(Y, a, b, r)\n",
    "\n",
    "gradient(Y, a, b, r)    # ANSWER: [[-6, 8, -8, -13]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17151a2c",
   "metadata": {},
   "source": [
    "#### iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e95357f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5.999998000660867,\n",
       " 8.000019001030978,\n",
       " -8.000000001118224,\n",
       " -12.999999999152578]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ iv\n",
    "#Finite Differences (Without - eps because there is no need to minus it as well.)\n",
    "def fin_diff(Y, a, b, r, eps=1e-6):             # NB: The unpacking of Y is done in the lagrangian function.\n",
    "    # Finite differences for X variables\n",
    "    grad_x = []\n",
    "    for i in range(len(Y)-1):                   # We iterate until the second last variable because after that is lambda\n",
    "\n",
    "        X_eps = Y.copy()                        # Reset array so we can perform a partial derivative\n",
    "        X_eps[i] = X_eps[i] + eps               # Increase ith variable by marginal amount\n",
    "        y_eps = lagrangian(X_eps, a, b, r)      # Perform lagrangian on new array\n",
    "        y = lagrangian(Y, a, b, r)              # Perform lagrangian on original array\n",
    "        grad_x.append((y_eps - y) / (eps))      # Calculate gradient between the two arrays\n",
    "\n",
    "    # Finite differences for lambda\n",
    "    lam_eps = Y.copy()\n",
    "    lam_eps[-1] = lam_eps[-1] + eps             # We use [-1] as this refers to our lambda value\n",
    "    y_eps = lagrangian(lam_eps, a, b, r)    \n",
    "    y = lagrangian(Y, a, b, r)\n",
    "    grad_lam = (y_eps - y) / (eps)\n",
    "\n",
    "    # Combine gradients into one array\n",
    "    grad = grad_x.copy()\n",
    "    grad.append(grad_lam)\n",
    "    return grad\n",
    "\n",
    "fin_diff([1,2,3,1], 1, 1, 1)\n",
    "# ANSWER = ([-5.9999, 8.00001, -8.000000001, -12.9999999]), which closely matches gradient function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c07c504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b = 1 : RMSE = 6.079735380977762e-06\n",
      "b = 100 : RMSE = 0.000546132799216091\n",
      "b = 1000 : RMSE = 0.005456641320609471\n"
     ]
    }
   ],
   "source": [
    "# Using RMSE to test the gradient function\n",
    "def RMSE(points, a, b, r):      # Points = array of coordinates\n",
    "    y = []          # Part iii jacobian gradient function output \n",
    "    y_hat = []      # Finite differences function output\n",
    "\n",
    "    # Calculate gradients\n",
    "    for point in points:\n",
    "        # Gradient/Jacobian function\n",
    "        output_jac = gradient(point, a, b, r)\n",
    "        y.append(output_jac)\n",
    "\n",
    "        # Finite differences function\n",
    "        output_fd = fin_diff(point, a, b, r)\n",
    "        y_hat.append(output_fd)\n",
    "\n",
    "    # RMSE Calculation\n",
    "        # Squared Error Calculation\n",
    "    squared_error = []\n",
    "    for k in range(K):\n",
    "        error = np.linalg.norm(y[k] - y_hat[k])\n",
    "        squared_error.append(error**2)\n",
    "\n",
    "        # Sum Squared Errors\n",
    "    sse = sum(squared_error)\n",
    "\n",
    "        # RMSE\n",
    "    RMSE = np.sqrt(sse/K)\n",
    "    return RMSE\n",
    "\n",
    "# Create points:\n",
    "K = 100    # Number of sampling points\n",
    "N = 6      # Number of variables + Lambda (To use as our argument vector)\n",
    "input_space = np.random.rand(K, N)\n",
    "\n",
    "# Find RMSE for different values of b\n",
    "B = [1, 100, 1000]\n",
    "for b_ in B:\n",
    "    output = RMSE(input_space, a, b_, r)\n",
    "    print(f\"b = {b_} : RMSE = {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293f074",
   "metadata": {},
   "source": [
    "As b increases, the RMSE increases somewhat linearly\n",
    "\n",
    "The intution is as follows:\n",
    "- As b increases, the function becomes steeper (i.e more sensitive to differences in the xi's).\n",
    "- This makes it more difficult for the finite differences method to approximate the gradient.\n",
    "- Ideally, a small change in xi should lead to an infinitesimal change in the rosenbrock function.\n",
    "- Increasing b makes a small change in xi lead to a larger change in the rosenbrock, which is suboptimal when trying to approximate the gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88923d3c",
   "metadata": {},
   "source": [
    "#### Is there a value of b where finite differences stops working well?\n",
    "- Even with b = 1000, the RMSE for 100 different points is very small.\n",
    "- We tested up to b = 100000, which produced an RMSE of approximately 0.5.\n",
    "- So we suggest that any value of b above 100000 makes finite differences not work so reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e6a6c",
   "metadata": {},
   "source": [
    "#### v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12aae314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.00000000e+00, -4.00000000e+00,  9.83591475e-17,\n",
       "        -2.00000000e+00],\n",
       "       [-4.00000000e+00,  3.80000000e+01, -8.00000000e+00,\n",
       "        -4.00000000e+00],\n",
       "       [ 9.83591475e-17, -8.00000000e+00,  0.00000000e+00,\n",
       "        -6.00000000e+00],\n",
       "       [-2.00000000e+00, -4.00000000e+00, -6.00000000e+00,\n",
       "         0.00000000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ v\n",
    "# From numdifftools, we can use the Hessian function\n",
    "def hessian(Y, a, b, r):\n",
    "    H = nd.Hessian(lagrangian)\n",
    "    return H(Y, a, b, r)\n",
    "\n",
    "hessian(Y,a,b,r)\n",
    "# ANSWER:\n",
    "# [[ 4, -4,  0, -2 ],\n",
    "#  [-4, 38, -8, -4 ],\n",
    "#  [ 0, -8,  0, -6 ],\n",
    "#  [-2, -4, -6,  0 ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808881ee",
   "metadata": {},
   "source": [
    "#### vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ff5205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.62903226],\n",
       "       [-0.61290323],\n",
       "       [-1.96774194],\n",
       "       [-0.51612903]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ vi\n",
    "def search_direction(Y, a, b, r):\n",
    "    H = hessian(Y,a,b,r)            # Generate Hessian\n",
    "    J = gradient(Y,a,b,r)           # Generate Jacobian\n",
    "    J = np.reshape(J, (len(Y),1))   # We need to transform the jacobian into a column vector in order to use linalg.solve\n",
    "\n",
    "    s_k = np.linalg.solve(H,-J)     # Ensure we input the negative of the Jacobian\n",
    "    return s_k\n",
    "\n",
    "search_direction(Y, a, b, r)\n",
    "# ANSWER\n",
    "# [[ 0.62903226],\n",
    "#  [-0.61290323],\n",
    "#  [-1.96774194],\n",
    "#  [-0.51612903]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad9d868",
   "metadata": {},
   "source": [
    "#### vii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a947ed40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.62903226],\n",
       "       [1.38709677],\n",
       "       [1.03225806],\n",
       "       [0.48387097]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ vii\n",
    "def iterate(Y_k, a, b, r, alpha=1):\n",
    "    s_k = search_direction(Y_k,a,b,r)               # Generate search direction\n",
    "    Y_k = np.reshape(np.array(Y_k), (len(Y_k),1))   # Ensuring that Y_k and s_k are the same shape\n",
    "    s_k = np.reshape(s_k, (len(s_k),1))             # Ensuring that Y_k and s_k are the same shape\n",
    "    Y_k_1 = Y_k + alpha*s_k             # Adding them together elementwise is easier when they are the same shape\n",
    "    return Y_k_1\n",
    "\n",
    "iterate(Y, a, b, r)\n",
    "# ANSWER:\n",
    "# [[1.62903226],\n",
    "#  [1.38709677],\n",
    "#  [1.03225806],\n",
    "#  [0.48387097]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e477d",
   "metadata": {},
   "source": [
    "#### viii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e767bb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ viii\n",
    "def convergence(Y, a, b, r, tol=1e-6):\n",
    "    norm = np.linalg.norm(gradient(Y,a,b,r))\n",
    "    return norm < tol\n",
    "\n",
    "convergence(Y,a,b,r)\n",
    "# ANSWER: False (which is expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90513e5f",
   "metadata": {},
   "source": [
    "#### ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0d91b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iterations: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.0586533664232292e+18, 0.0]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ ix\n",
    "# Set parameters\n",
    "a = 1\n",
    "b = 1\n",
    "r = 2\n",
    "\n",
    "x_k = []    # Create empty list\n",
    "def newton(Y_0, a, b, r):\n",
    "    iter = 0\n",
    "    y_k = Y_0\n",
    "    while not convergence(y_k, a, b, r):\n",
    "        y_k_1 = iterate(y_k, a, b, r)\n",
    "\n",
    "        y_k = y_k_1.tolist()    # y_k_1 is an np.array but we need to change the input to a list as our functions only take in lists\n",
    "        y_k = sum(y_k, [])      # This line converts y_k from a list of lists to a single list \n",
    "        x_k.append(y_k[0:-1])   # Make sure to not add the value of lambda to the list of x_k\n",
    "        iter += 1\n",
    "    print(f\"number of iterations: {iter}\")\n",
    "\n",
    "newton([0,0,0], a, b, r)\n",
    "x_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8330c585",
   "metadata": {},
   "source": [
    "Unfortunately our newton function would not work for the points [0,0,0]\n",
    "\n",
    "After some debugging, we find that the linalg.solve function in our search_direction function returns very high values (up to 1e36).\n",
    "\n",
    "These numbers are too high when called by functions like the jacobian, which seems to break and automatically outputs a value of [0,0,0], as if the root has been found precisely.\n",
    "\n",
    "As a result, our convergence checker returns True after 1 itertation and the function ends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd32d05e",
   "metadata": {},
   "source": [
    "#### x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f8f5ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ x\n",
    "def newton_complete(Y_0, a, b, r, tol=1e-6, max_iterations=1000):     # Parameter n not needed as our functions will get n from the length of Y_0\n",
    "    # Initialize values\n",
    "    iter = 0 \n",
    "    y_k = Y_0\n",
    "    \n",
    "    # Newton loop:\n",
    "    while not (convergence(y_k, a, b, r, tol)) or (iter < max_iterations):\n",
    "        y_k_1 = iterate(y_k, a, b, r)\n",
    "\n",
    "        y_k = y_k_1.tolist()    # y_k_1 is an np.array but we need to change the input to a list as our functions only take in lists\n",
    "        y_k = sum(y_k, [])      # This line converts y_k from a list of lists to a single list\n",
    "        iter += 1\n",
    "\n",
    "    # Create values to return:\n",
    "    f_value = rosen(y_k, a, b)\n",
    "    optimum_x = y_k[0:-1]\n",
    "    optimum_lam = y_k[-1]\n",
    "    newton_steps = iter\n",
    "    norm_DL = np.linalg.norm(gradient(y_k,a,b,r))\n",
    "    cons_deviation = constraint(y_k[0:-1], r)\n",
    "\n",
    "    return {\"f_value\":f_value, \"optimum_x\":optimum_x, \"optimum_lam\":optimum_lam, \"newton_steps\":newton_steps, \"norm_DL\":norm_DL, \"cons_deviation\":cons_deviation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c310407",
   "metadata": {},
   "source": [
    "#### xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20e79590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Initial point 1': {'f_value': 1.3358479582605698,\n",
       "   'optimum_x': [0.7990113591987212,\n",
       "    0.7430214763780109,\n",
       "    0.6799183498147472,\n",
       "    0.5512916025361428,\n",
       "    0.20805898219201105],\n",
       "   'optimum_lam': -0.4607513111180575,\n",
       "   'newton_steps': 100,\n",
       "   'norm_DL': 7.043923318186837e-16,\n",
       "   'cons_deviation': -2.220446049250313e-16}},\n",
       " {'Initial point 2': {'f_value': 1.3358479582605693,\n",
       "   'optimum_x': [0.7990113591987212,\n",
       "    0.7430214763780109,\n",
       "    0.6799183498147472,\n",
       "    0.5512916025361428,\n",
       "    0.20805898219201102],\n",
       "   'optimum_lam': -0.4607513111180572,\n",
       "   'newton_steps': 100,\n",
       "   'norm_DL': 7.098592150026709e-16,\n",
       "   'cons_deviation': -2.220446049250313e-16}},\n",
       " {'Initial point 3': {'f_value': 1.335847958260569,\n",
       "   'optimum_x': [0.7990113591987211,\n",
       "    0.7430214763780107,\n",
       "    0.6799183498147473,\n",
       "    0.551291602536143,\n",
       "    0.20805898219201113],\n",
       "   'optimum_lam': -0.46075131111805656,\n",
       "   'newton_steps': 100,\n",
       "   'norm_DL': 4.749397776179922e-15,\n",
       "   'cons_deviation': 0.0}},\n",
       " {'Initial point 4': {'f_value': 1.3358479582605698,\n",
       "   'optimum_x': [0.7990113591987212,\n",
       "    0.7430214763780107,\n",
       "    0.6799183498147471,\n",
       "    0.5512916025361427,\n",
       "    0.2080589821920109],\n",
       "   'optimum_lam': -0.4607513111180572,\n",
       "   'newton_steps': 100,\n",
       "   'norm_DL': 1.4574444538542914e-15,\n",
       "   'cons_deviation': -6.661338147750939e-16}},\n",
       " {'Initial point 5': {'f_value': 1.335847958260568,\n",
       "   'optimum_x': [0.7990113591987216,\n",
       "    0.7430214763780112,\n",
       "    0.6799183498147475,\n",
       "    0.5512916025361432,\n",
       "    0.2080589821920111],\n",
       "   'optimum_lam': -0.46075131111805684,\n",
       "   'newton_steps': 100,\n",
       "   'norm_DL': 1.5950972697814858e-15,\n",
       "   'cons_deviation': 1.7763568394002505e-15}}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ xi\n",
    "r = 2\n",
    "\n",
    "# Create points:\n",
    "K = 5   # Number of initial points\n",
    "N = 6   # Number of variables + Lambda (To use as our argument vector)\n",
    "\n",
    "output = []\n",
    "for k in range(K):\n",
    "    Y_0 = np.random.uniform(0, 10, N)\n",
    "    output.append({f\"Initial point {k+1}\" : newton_complete(Y_0, a, b, r, max_iterations=100)})\n",
    "    \n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427dbd39",
   "metadata": {},
   "source": [
    "\n",
    "Overall, the function does not seem to be working as intended. We think it may have to do with our linalg.solve function.\n",
    "\n",
    "Perhaps the fact that lambda is included in our Jacobian and Hessian may be confusing the linalg.solve function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b633660",
   "metadata": {},
   "source": [
    "### 1(b) Penalty method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e1294de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions:\n",
    "def rosen_b(X):\n",
    "    f = []\n",
    "    for i in range(len(X)-1):\n",
    "        f_i = (a*(1-X[i])**2) + b*(X[i+1] - X[i]**2)**2\n",
    "        f.append(f_i)\n",
    "    return sum(f)\n",
    "\n",
    "def g(X,r):\n",
    "    g = []\n",
    "    for i in range(len(X)):\n",
    "        g_i = X[i]**2\n",
    "        g.append(g_i)\n",
    "    return sum(g)-r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b82f8c",
   "metadata": {},
   "source": [
    "#### i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b8a5245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Objective value (How well L-BFGS minimizes the function): 90.0\n",
      "Constraint value (How good the optimal solution satisfies the constraint): -10.0\n",
      "Number of iterations: 0\n"
     ]
    }
   ],
   "source": [
    "# (i)\n",
    "\n",
    "a = 10\n",
    "b = 10\n",
    "r = 10\n",
    "n = 10\n",
    "Pk = 1e12\n",
    "x0= np.zeros(n)\n",
    "\n",
    "def penalty_method(X, Pk):\n",
    "    return rosen_b(X) + (Pk*np.linalg.norm(g(X,r)))\n",
    "\n",
    "res = minimize(penalty_method, x0, args=(Pk), method='L-BFGS-B', options={'maxiter': 10000})\n",
    "\n",
    "print(\"Optimal solution:\\n\", res.x)\n",
    "print(\"Objective value (How well L-BFGS minimizes the function):\", rosen_b(res.x))\n",
    "print(\"Constraint value (How good the optimal solution satisfies the constraint):\", g(res.x,r))\n",
    "print(\"Number of iterations:\", res.nit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f2adb8",
   "metadata": {},
   "source": [
    "Our minimize function doesn't seem to work $x_0$ = 0. As can be seen from the number of iterations being 0.\n",
    "\n",
    "We think this is because the penalty function is too high.\n",
    "\n",
    "We will see in later parts that the iterative Pk method fares much better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260e7f8",
   "metadata": {},
   "source": [
    "#### ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "239932a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 1\n",
      "Optimal solution:\n",
      " [0.99999413 0.99999299 0.99998925 0.99999446 1.00000078 1.0000073\n",
      " 1.00001029 1.00000497 1.00000163 1.00000419]\n",
      "Objective value: 1.2107985907919064e-08\n",
      "Constraint value: -3.604421650038603e-09\n",
      "Number of iterations: 33\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 10\n",
      "Optimal solution:\n",
      " [0.99999413 0.99999299 0.99998925 0.99999446 1.00000078 1.0000073\n",
      " 1.00001029 1.00000497 1.00000163 1.00000419]\n",
      "Objective value: 1.2107985907919064e-08\n",
      "Constraint value: -3.604421650038603e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 100\n",
      "Optimal solution:\n",
      " [0.99999413 0.99999299 0.99998925 0.99999446 1.00000078 1.0000073\n",
      " 1.00001029 1.00000497 1.00000163 1.00000419]\n",
      "Objective value: 1.2107985907919064e-08\n",
      "Constraint value: -3.604421650038603e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 1000\n",
      "Optimal solution:\n",
      " [0.99999413 0.99999299 0.99998925 0.99999446 1.00000078 1.0000073\n",
      " 1.00001029 1.00000497 1.00000163 1.00000419]\n",
      "Objective value: 1.2107985907919064e-08\n",
      "Constraint value: -3.604421650038603e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 10000\n",
      "Optimal solution:\n",
      " [0.99999413 0.99999299 0.99998925 0.99999446 1.00000078 1.0000073\n",
      " 1.00001029 1.00000497 1.00000163 1.00000419]\n",
      "Objective value: 1.2107985907919064e-08\n",
      "Constraint value: -3.604421650038603e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 100000\n",
      "Optimal solution:\n",
      " [0.99999413 0.99999299 0.99998925 0.99999446 1.00000078 1.0000073\n",
      " 1.00001029 1.00000497 1.00000163 1.00000419]\n",
      "Objective value: 1.2107985907919064e-08\n",
      "Constraint value: -3.604421650038603e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 1000000\n",
      "Optimal solution:\n",
      " [0.99999413 0.99999299 0.99998925 0.99999446 1.00000078 1.0000073\n",
      " 1.00001029 1.00000497 1.00000163 1.00000419]\n",
      "Objective value: 1.2107985907919064e-08\n",
      "Constraint value: -3.604421650038603e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 10000000\n",
      "Optimal solution:\n",
      " [0.99999413 0.99999299 0.99998925 0.99999446 1.00000078 1.0000073\n",
      " 1.00001029 1.00000497 1.00000163 1.00000419]\n",
      "Objective value: 1.2107985907919064e-08\n",
      "Constraint value: -3.604421650038603e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 100000000\n",
      "Optimal solution:\n",
      " [0.99999413 0.99999299 0.99998925 0.99999446 1.00000078 1.0000073\n",
      " 1.00001029 1.00000497 1.00000163 1.00000419]\n",
      "Objective value: 1.2107985907919064e-08\n",
      "Constraint value: -3.604421650038603e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 1000000000\n",
      "Optimal solution:\n",
      " [0.99999413 0.99999299 0.99998925 0.99999446 1.00000078 1.0000073\n",
      " 1.00001029 1.00000497 1.00000163 1.00000419]\n",
      "Objective value: 1.2107985907919064e-08\n",
      "Constraint value: -3.604421650038603e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 10000000000\n",
      "Optimal solution:\n",
      " [0.99999413 0.99999299 0.99998925 0.99999446 1.00000078 1.0000073\n",
      " 1.00001029 1.00000497 1.00000163 1.00000419]\n",
      "Objective value: 1.2107985907919064e-08\n",
      "Constraint value: -3.604421650038603e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 100000000000\n",
      "Optimal solution:\n",
      " [0.99999413 0.99999299 0.99998925 0.99999446 1.00000078 1.0000073\n",
      " 1.00001029 1.00000497 1.00000163 1.00000419]\n",
      "Objective value: 1.2107985907919064e-08\n",
      "Constraint value: -3.604421650038603e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 1000000000000\n",
      "Optimal solution:\n",
      " [0.99999413 0.99999299 0.99998925 0.99999446 1.00000078 1.0000073\n",
      " 1.00001029 1.00000497 1.00000163 1.00000419]\n",
      "Objective value: 1.2107985907919064e-08\n",
      "Constraint value: -3.604421650038603e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# (ii)\n",
    "\n",
    "Pk = 1\n",
    "x0 = np.zeros(n)\n",
    "for i in range(13):\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(f\"Testing Penalty Method with Pk = {Pk}\")\n",
    "    res = minimize(penalty_method, x0, args=(Pk), method='L-BFGS-B', options={'maxiter': 10000})\n",
    "\n",
    "    print(\"Optimal solution:\\n\", res.x)\n",
    "    print(\"Objective value:\", rosen_b(res.x))\n",
    "    print(\"Constraint value:\", g(res.x,r))\n",
    "    print(\"Number of iterations:\", res.nit)\n",
    "    \n",
    "    x0 = res.x    # Set xk+1 as new solution\n",
    "    Pk *= 10      # Pk+1 = Pk*10\n",
    "\n",
    "    print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84a840",
   "metadata": {},
   "source": [
    "With this method, our results fare much better.\n",
    "\n",
    "Starting with Pk=1, the minimize function has a bit more room to change values.\n",
    "\n",
    "We see that combining an iterative increase in Pk with the assignment of $x_{k+1}$ as the previous solution allows us to get much closer to the true optimum values.\n",
    "\n",
    "However, after the first iteration, the solution is already close enough to the true values, such that the minimize function - which takes in this value as an input - is already satisfied that the optimum solution has been reached. So there isn't much need for many iterations through Pk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb9dcc",
   "metadata": {},
   "source": [
    "#### iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47231d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 1\n",
      "Optimal solution:\n",
      " [1.00000217 1.00000007 0.99999929 0.99999863 1.00000213 1.00000216\n",
      " 1.000002   1.00000158 0.99999679 0.99999517]\n",
      "Objective value: 1.0195622143936843e-07\n",
      "Constraint value: -1.1371737329568532e-08\n",
      "Number of iterations: 70\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 10\n",
      "Optimal solution:\n",
      " [1.00000217 1.00000007 0.99999929 0.99999863 1.00000213 1.00000216\n",
      " 1.000002   1.00000158 0.99999679 0.99999517]\n",
      "Objective value: 1.0195911085323836e-07\n",
      "Constraint value: -8.869506018527318e-09\n",
      "Number of iterations: 2\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 100\n",
      "Optimal solution:\n",
      " [1.00000217 1.00000007 0.99999929 0.99999863 1.00000213 1.00000216\n",
      " 1.000002   1.00000158 0.99999679 0.99999517]\n",
      "Objective value: 1.0195911085323836e-07\n",
      "Constraint value: -8.869506018527318e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 1000\n",
      "Optimal solution:\n",
      " [1.00000217 1.00000007 0.99999929 0.99999863 1.00000213 1.00000216\n",
      " 1.000002   1.00000158 0.99999679 0.99999517]\n",
      "Objective value: 1.0195911085323836e-07\n",
      "Constraint value: -8.869506018527318e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 10000\n",
      "Optimal solution:\n",
      " [1.00000217 1.00000007 0.99999929 0.99999863 1.00000213 1.00000216\n",
      " 1.000002   1.00000158 0.99999679 0.99999517]\n",
      "Objective value: 1.0195911085323836e-07\n",
      "Constraint value: -8.869506018527318e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 100000\n",
      "Optimal solution:\n",
      " [1.00000217 1.00000007 0.99999929 0.99999863 1.00000213 1.00000216\n",
      " 1.000002   1.00000158 0.99999679 0.99999517]\n",
      "Objective value: 1.0195911085323836e-07\n",
      "Constraint value: -8.869506018527318e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 1000000\n",
      "Optimal solution:\n",
      " [1.00000217 1.00000007 0.99999929 0.99999863 1.00000213 1.00000216\n",
      " 1.000002   1.00000158 0.99999679 0.99999517]\n",
      "Objective value: 1.0195911085323836e-07\n",
      "Constraint value: -8.869506018527318e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 10000000\n",
      "Optimal solution:\n",
      " [1.00000217 1.00000007 0.99999929 0.99999863 1.00000213 1.00000216\n",
      " 1.000002   1.00000158 0.99999679 0.99999517]\n",
      "Objective value: 1.0195911085323836e-07\n",
      "Constraint value: -8.869506018527318e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 100000000\n",
      "Optimal solution:\n",
      " [1.00000217 1.00000007 0.99999929 0.99999863 1.00000213 1.00000216\n",
      " 1.000002   1.00000158 0.99999679 0.99999517]\n",
      "Objective value: 1.0195911085323836e-07\n",
      "Constraint value: -8.869506018527318e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 1000000000\n",
      "Optimal solution:\n",
      " [1.00000217 1.00000007 0.99999929 0.99999863 1.00000213 1.00000216\n",
      " 1.000002   1.00000158 0.99999679 0.99999517]\n",
      "Objective value: 1.0195911085323836e-07\n",
      "Constraint value: -8.869506018527318e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 10000000000\n",
      "Optimal solution:\n",
      " [1.00000217 1.00000007 0.99999929 0.99999863 1.00000213 1.00000216\n",
      " 1.000002   1.00000158 0.99999679 0.99999517]\n",
      "Objective value: 1.0195911085323836e-07\n",
      "Constraint value: -8.869506018527318e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 100000000000\n",
      "Optimal solution:\n",
      " [1.00000217 1.00000007 0.99999929 0.99999863 1.00000213 1.00000216\n",
      " 1.000002   1.00000158 0.99999679 0.99999517]\n",
      "Objective value: 1.0195911085323836e-07\n",
      "Constraint value: -8.869506018527318e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 1000000000000\n",
      "Optimal solution:\n",
      " [1.00000217 1.00000007 0.99999929 0.99999863 1.00000213 1.00000216\n",
      " 1.000002   1.00000158 0.99999679 0.99999517]\n",
      "Objective value: 1.0195911085323836e-07\n",
      "Constraint value: -8.869506018527318e-09\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# (iii)\n",
    "\n",
    "Pk = 1\n",
    "b = 1000\n",
    "x0 = np.zeros(n)\n",
    "for i in range(13):\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(f\"Testing Penalty Method with Pk = {Pk}\")\n",
    "    res = minimize(penalty_method, x0, args=(Pk), method='L-BFGS-B', options={'maxiter': 10000})\n",
    "\n",
    "    print(\"Optimal solution:\\n\", res.x)\n",
    "    print(\"Objective value:\", rosen_b(res.x))\n",
    "    print(\"Constraint value:\", g(res.x,r))\n",
    "    print(\"Number of iterations:\", res.nit)\n",
    "    \n",
    "    x0 = res.x    # Set xk+1 as new solution\n",
    "    Pk *= 10      # Pk+1 = Pk*10\n",
    "\n",
    "    print(\"---------------------------------------------------------------\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ead3e",
   "metadata": {},
   "source": [
    "Now that b = 1000, convergence takes longer.\n",
    "\n",
    "In previous questions, we talked about how increasing b makes the rosenbrock function more steep, which generally makes optimization more difficult. So multiplying b by a factor of 100 is probably the reason why we have slower convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af2c51d",
   "metadata": {},
   "source": [
    "#### iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2142b33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 1\n",
      "Optimal solution:\n",
      " [0.96107433 0.95341207 0.95016342 0.94824843 0.94584292 0.94065447\n",
      " 0.92775836 0.89494054 0.81134143 0.5984372 ]\n",
      "Objective value: 0.8279437620874046\n",
      "Constraint value: 7.092149843537962\n",
      "Number of iterations: 15\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 10\n",
      "Optimal solution:\n",
      " [0.60316994 0.53485877 0.51114339 0.50323287 0.50007739 0.49703284\n",
      " 0.48950058 0.46582105 0.38642832 0.07466313]\n",
      "Objective value: 26.89529967455445\n",
      "Constraint value: 1.2730192853909386\n",
      "Number of iterations: 10\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 100\n",
      "Optimal solution:\n",
      " [0.39795164 0.35177078 0.33780941 0.33315482 0.3312991  0.32950774\n",
      " 0.3250781  0.3111596  0.2645622  0.04428101]\n",
      "Objective value: 43.79381061944943\n",
      "Constraint value: -8.319148703250789e-10\n",
      "Number of iterations: 6\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 1000\n",
      "Optimal solution:\n",
      " [0.39795164 0.35177078 0.33780941 0.33315482 0.3312991  0.32950774\n",
      " 0.3250781  0.3111596  0.2645622  0.04428101]\n",
      "Objective value: 43.79381061944943\n",
      "Constraint value: -8.319148703250789e-10\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 10000\n",
      "Optimal solution:\n",
      " [0.39795164 0.35177078 0.33780941 0.33315482 0.3312991  0.32950774\n",
      " 0.3250781  0.3111596  0.2645622  0.04428101]\n",
      "Objective value: 43.79381061944943\n",
      "Constraint value: -8.319148703250789e-10\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 100000\n",
      "Optimal solution:\n",
      " [0.39795164 0.35177078 0.33780941 0.33315482 0.3312991  0.32950774\n",
      " 0.3250781  0.3111596  0.2645622  0.04428101]\n",
      "Objective value: 43.79381061944943\n",
      "Constraint value: -8.319148703250789e-10\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 1000000\n",
      "Optimal solution:\n",
      " [0.39795164 0.35177078 0.33780941 0.33315482 0.3312991  0.32950774\n",
      " 0.3250781  0.3111596  0.2645622  0.04428101]\n",
      "Objective value: 43.79381061944943\n",
      "Constraint value: -8.319148703250789e-10\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 10000000\n",
      "Optimal solution:\n",
      " [0.39795164 0.35177078 0.33780941 0.33315482 0.3312991  0.32950774\n",
      " 0.3250781  0.3111596  0.2645622  0.04428101]\n",
      "Objective value: 43.79381061944943\n",
      "Constraint value: -8.319148703250789e-10\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 100000000\n",
      "Optimal solution:\n",
      " [0.39795164 0.35177078 0.33780941 0.33315482 0.3312991  0.32950774\n",
      " 0.3250781  0.3111596  0.2645622  0.04428101]\n",
      "Objective value: 43.79381061944943\n",
      "Constraint value: -8.319148703250789e-10\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 1000000000\n",
      "Optimal solution:\n",
      " [0.39795164 0.35177078 0.33780941 0.33315482 0.3312991  0.32950774\n",
      " 0.3250781  0.3111596  0.2645622  0.04428101]\n",
      "Objective value: 43.79381061944943\n",
      "Constraint value: -8.319148703250789e-10\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 10000000000\n",
      "Optimal solution:\n",
      " [0.39795164 0.35177078 0.33780941 0.33315482 0.3312991  0.32950774\n",
      " 0.3250781  0.3111596  0.2645622  0.04428101]\n",
      "Objective value: 43.79381061944943\n",
      "Constraint value: -8.319148703250789e-10\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 100000000000\n",
      "Optimal solution:\n",
      " [0.39795164 0.35177078 0.33780941 0.33315482 0.3312991  0.32950774\n",
      " 0.3250781  0.3111596  0.2645622  0.04428101]\n",
      "Objective value: 43.79381061944943\n",
      "Constraint value: -8.319148703250789e-10\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Testing Penalty Method with Pk = 1000000000000\n",
      "Optimal solution:\n",
      " [0.39795164 0.35177078 0.33780941 0.33315482 0.3312991  0.32950774\n",
      " 0.3250781  0.3111596  0.2645622  0.04428101]\n",
      "Objective value: 43.79381061944943\n",
      "Constraint value: -8.319148703250789e-10\n",
      "Number of iterations: 0\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# (iv)\n",
    "\n",
    "Pk = 1\n",
    "b = 10 # We chose to set b back to 10 for this part\n",
    "r = 1\n",
    "x0 = np.zeros(n)\n",
    "for i in range(13):\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(f\"Testing Penalty Method with Pk = {Pk}\")\n",
    "    res = minimize(penalty_method, x0, args=(Pk), method='L-BFGS-B', options={'maxiter': 10000})\n",
    "\n",
    "    print(\"Optimal solution:\\n\", res.x)\n",
    "    print(\"Objective value:\", rosen_b(res.x))\n",
    "    print(\"Constraint value:\", g(res.x,r))\n",
    "    print(\"Number of iterations:\", res.nit)\n",
    "    \n",
    "    x0 = res.x    # Set xk+1 as new solution\n",
    "    Pk *= 10      # Pk+1 = Pk*10\n",
    "\n",
    "    print(\"---------------------------------------------------------------\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff719f",
   "metadata": {},
   "source": [
    "Now that r = 1 (and b is back to 10), we see that the function does not converge to a matrix of ones.\n",
    "\n",
    "We do believe the function is converging correctly. But a convergence to a different value suggests that the global minimum does not occur within the specified constraint.\n",
    "\n",
    "We think that this function has found the local minimum of the rosenbrock that is constrained by a unit sphere of radius r.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c855bc82",
   "metadata": {},
   "source": [
    "### 1(c) Augmented Lagrangian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a73ee49",
   "metadata": {},
   "source": [
    "### 1(d) Parameter Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e29d8173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution:\n",
      " [0.99999999 0.99999999 0.99999998 0.99999999 0.99999996 0.99999998\n",
      " 0.99999998 0.99999997 0.99999995 0.99999988]\n",
      "Objective value: 1.1262388848283833e-13\n",
      "Constraint value: 8.999999326045133\n",
      "Number of iterations: 24\n"
     ]
    }
   ],
   "source": [
    "# define function hi(z)\n",
    "a = 10\n",
    "b = 10\n",
    "def h (z,i,n):\n",
    "    if i<= n-1:\n",
    "        num = np.exp(z[i])\n",
    "        den = 1 + sum(np.exp(z[s]) for s in range(s-1))        \n",
    "        return np.sqrt(num/den)    \n",
    "    else: \n",
    "        return np.sqrt(1/den)\n",
    "\n",
    "# define inverse function hi^-1(x)\n",
    "def h_inverse (x):\n",
    "    xi = x[-1]\n",
    "    xn = x[:-1]\n",
    "    log_thing = np.log(xi/xn)\n",
    "    return 2*log_thing\n",
    "\n",
    "# to solve, substitue inverse function into constraint part of multiplier so it becomes, lagrange = (a(1-x[i])**2 + b(x[i+1]-x[i]**2)**2) + lamda (h_inverse)\n",
    "\n",
    "x0 = np.zeros(n)\n",
    "res = minimize(rosen_b, x0, method='BFGS') \n",
    "\n",
    "print(\"Optimal solution:\\n\", res.x)\n",
    "print(\"Objective value:\", rosen_b(res.x))\n",
    "print(\"Constraint value:\", g(res.x,r))\n",
    "print(\"Number of iterations:\", res.nit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0754e484",
   "metadata": {},
   "source": [
    "This method works well.\n",
    "\n",
    "It is much better than our approach in part a) since it correctly converges and is reliable.\n",
    "\n",
    "Compared to our approach in b):\n",
    "- It seems to be more reliable. It does not require us to choose a penalty value, which may or may not break break the function.\n",
    "- It is faster. It took 24 iterations, while our most ideal scenario in part b) took 33 iterations.\n",
    "- It is as accurate as our most ideal scenario in part b), but of course this is subject to our function in part b working properly.\n",
    "\n",
    "Overall, parameter transformation seems to be the best way of finding the global mimimum of our rosenbrock function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
